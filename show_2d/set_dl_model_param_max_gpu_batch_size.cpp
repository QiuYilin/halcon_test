///////////////////////////////////////////////////////////////////////////////
// File generated by HDevelop for HALCON/C++ Version 23.05.0.0
// Non-ASCII strings in this file are encoded in local-8-bit encoding (cp936).
// Ensure that the interface encoding is set to locale encoding by calling
// SetHcppInterfaceStringEncodingIsUtf8(false) at the beginning of the program.
// 
// Please note that non-ASCII characters in string constants are exported
// as octal codes in order to guarantee that the strings are correctly
// created on all systems, independent on any compiler settings.
// 
// Source files with different encoding should not be mixed in one project.
///////////////////////////////////////////////////////////////////////////////

#include "HalconCpp.h"
#include "HDevThread.h"



using namespace HalconCpp;

// Procedure declarations 
// Chapter: Deep Learning / Model
// Short Description: Create blank train sample dictionaries for a given model. 
void gen_blank_dl_train_samples (HTuple hv_DLModelHandle, HTuple *hv_TrainSamples);
// Chapter: Deep Learning / Model
// Short Description: Set the maximum batch size for a given DLModelHandle and GPU. 
void set_dl_model_param_max_gpu_batch_size (HTuple hv_DLModelHandle, HTuple hv_BatchSizeUpperBound);

// Procedures 
// Chapter: Deep Learning / Model
// Short Description: Create blank train sample dictionaries for a given model. 
void gen_blank_dl_train_samples (HTuple hv_DLModelHandle, HTuple *hv_TrainSamples)
{

  // Local iconic variables
  HObject  ho_Images, ho_Image, ho_EmptyObject;
  HObject  ho_ConstImage, ho_ConstImage2, ho_Target, ho_Weight;

  // Local control variables
  HTuple  hv_ModelType, hv_BatchSize, hv_ImageWidth;
  HTuple  hv_ImageHeight, hv_ImageNumChannels, hv_ClassIDs;
  HTuple  hv_I, hv_Index, hv_TrainInput, hv_InstanceType;
  HTuple  hv_Exception, hv_InstanceSegmentation, hv_Alphabet;

  //
  //This procedure generates blank input data for the given model.
  //
  //Get model parameters.
  GetDlModelParam(hv_DLModelHandle, "type", &hv_ModelType);
  GetDlModelParam(hv_DLModelHandle, "batch_size", &hv_BatchSize);
  GetDlModelParam(hv_DLModelHandle, "image_width", &hv_ImageWidth);
  GetDlModelParam(hv_DLModelHandle, "image_height", &hv_ImageHeight);
  GetDlModelParam(hv_DLModelHandle, "image_num_channels", &hv_ImageNumChannels);
  if (0 != (HTuple(HTuple(int(hv_ModelType!=HTuple("gc_anomaly_detection"))).TupleAnd(int(hv_ModelType!=HTuple("ocr_recognition")))).TupleAnd(int(hv_ModelType!=HTuple("ocr_detection")))))
  {
    GetDlModelParam(hv_DLModelHandle, "class_ids", &hv_ClassIDs);
  }
  //
  //Collect train inputs.
  (*hv_TrainSamples) = HTuple();
  //
  {
  HTuple end_val16 = hv_BatchSize-1;
  HTuple step_val16 = 1;
  for (hv_I=0; hv_I.Continue(end_val16, step_val16); hv_I += step_val16)
  {
    //Create blank image.
    GenEmptyObj(&ho_Images);
    {
    HTuple end_val19 = hv_ImageNumChannels-1;
    HTuple step_val19 = 1;
    for (hv_Index=0; hv_Index.Continue(end_val19, step_val19); hv_Index += step_val19)
    {
      GenImageConst(&ho_Image, "real", hv_ImageWidth, hv_ImageHeight);
      ConcatObj(ho_Images, ho_Image, &ho_Images);
    }
    }
    ChannelsToImage(ho_Images, &ho_Image);
    //
    //Create train data.
    CreateDict(&hv_TrainInput);
    SetDictObject(ho_Image, hv_TrainInput, "image");
    //
    //Add model specific blank training data.
    if (0 != (int(hv_ModelType==HTuple("classification"))))
    {
      //Assign arbitrary label for image.
      SetDictTuple(hv_TrainInput, "image_label_id", 0);
    }
    else if (0 != (int(hv_ModelType==HTuple("detection"))))
    {
      //Generate blank bounding box labels depending on the instance type.
      hv_InstanceType = "rectangle1";
      try
      {
        GetDlModelParam(hv_DLModelHandle, "instance_type", &hv_InstanceType);
      }
      // catch (Exception) 
      catch (HException &HDevExpDefaultException)
      {
        HDevExpDefaultException.ToHTuple(&hv_Exception);
      }
      if (0 != (int(hv_InstanceType==HTuple("rectangle1"))))
      {
        SetDictTuple(hv_TrainInput, "bbox_label_id", HTuple());
        SetDictTuple(hv_TrainInput, "bbox_row1", HTuple());
        SetDictTuple(hv_TrainInput, "bbox_row2", HTuple());
        SetDictTuple(hv_TrainInput, "bbox_col1", HTuple());
        SetDictTuple(hv_TrainInput, "bbox_col2", HTuple());
      }
      else if (0 != (int(hv_InstanceType==HTuple("rectangle2"))))
      {
        SetDictTuple(hv_TrainInput, "bbox_label_id", HTuple());
        SetDictTuple(hv_TrainInput, "bbox_row", HTuple());
        SetDictTuple(hv_TrainInput, "bbox_col", HTuple());
        SetDictTuple(hv_TrainInput, "bbox_length1", HTuple());
        SetDictTuple(hv_TrainInput, "bbox_length2", HTuple());
        SetDictTuple(hv_TrainInput, "bbox_phi", HTuple());
      }
      else
      {
        throw HException(("The current instance type is not supported: \""+hv_InstanceType)+"\"");
      }
      hv_InstanceSegmentation = 0;
      try
      {
        GetDlModelParam(hv_DLModelHandle, "instance_segmentation", &hv_InstanceSegmentation);
        hv_InstanceSegmentation = int(hv_InstanceSegmentation==HTuple("true"));
      }
      // catch (Exception) 
      catch (HException &HDevExpDefaultException)
      {
        HDevExpDefaultException.ToHTuple(&hv_Exception);
      }
      if (0 != hv_InstanceSegmentation)
      {
        GenEmptyObj(&ho_EmptyObject);
        SetDictObject(ho_EmptyObject, hv_TrainInput, "mask");
      }
    }
    else if (0 != (int(hv_ModelType==HTuple("gc_anomaly_detection"))))
    {
      //Nothing has to be done.
    }
    else if (0 != (int(hv_ModelType==HTuple("ocr_recognition"))))
    {
      //Generate dummy word label with the first alphabet entry.
      GetDlModelParam(hv_DLModelHandle, "alphabet", &hv_Alphabet);
      SetDictTuple(hv_TrainInput, "word", HTuple(hv_Alphabet[0]));
    }
    else if (0 != (int(hv_ModelType==HTuple("ocr_detection"))))
    {
      SetDictTuple(hv_TrainInput, "bbox_label_id", HTuple());
      SetDictTuple(hv_TrainInput, "bbox_row", HTuple());
      SetDictTuple(hv_TrainInput, "bbox_col", HTuple());
      SetDictTuple(hv_TrainInput, "bbox_length1", HTuple());
      SetDictTuple(hv_TrainInput, "bbox_length2", HTuple());
      SetDictTuple(hv_TrainInput, "bbox_phi", HTuple());
      SetDictTuple(hv_TrainInput, "word", HTuple());
      GenImageConst(&ho_ConstImage, "real", hv_ImageWidth/2, hv_ImageHeight/2);
      Compose2(ho_ConstImage, ho_ConstImage, &ho_ConstImage2);
      SetDictObject(ho_ConstImage, hv_TrainInput, "target_text");
      SetDictObject(ho_ConstImage, hv_TrainInput, "target_link");
      SetDictObject(ho_ConstImage2, hv_TrainInput, "target_orientation");
      SetDictObject(ho_ConstImage, hv_TrainInput, "target_weight_text");
      SetDictObject(ho_ConstImage, hv_TrainInput, "target_weight_link");
      SetDictObject(ho_ConstImage2, hv_TrainInput, "target_weight_orientation");
    }
    else if (0 != (int(hv_ModelType==HTuple("segmentation"))))
    {
      //Generate blank weights and targets.
      GenImageConst(&ho_Target, "real", hv_ImageWidth, hv_ImageHeight);
      GenImageConst(&ho_Weight, "real", hv_ImageWidth, hv_ImageHeight);
      //Fill with first color background.
      OverpaintRegion(ho_Target, ho_Target, HTuple(hv_ClassIDs[0]), "fill");
      OverpaintRegion(ho_Weight, ho_Weight, 1, "fill");
      SetDictObject(ho_Target, hv_TrainInput, "segmentation_image");
      SetDictObject(ho_Weight, hv_TrainInput, "weight_image");
    }
    else
    {
      throw HException(("The current model type is not supported: \""+hv_ModelType)+"\"");
    }
    (*hv_TrainSamples) = (*hv_TrainSamples).TupleConcat(hv_TrainInput);
  }
  }
  return;
}

// Chapter: Deep Learning / Model
// Short Description: Set the maximum batch size for a given DLModelHandle and GPU. 
void set_dl_model_param_max_gpu_batch_size (HTuple hv_DLModelHandle, HTuple hv_BatchSizeUpperBound)
{

  // Local iconic variables

  // Local control variables
  HTuple  hv_ModelType, hv_ModelTypesSupported;
  HTuple  hv_Index, hv_GpuID, hv_DLDeviceOrig, hv_DLDeviceGPU;
  HTuple  hv_DLDeviceCPU, hv_SerializedItemHandle, hv_OriginalModel;
  HTuple  hv_Exception, hv_BatchSizeTestL, hv_BatchSizeTestR;
  HTuple  hv_BatchSizeTest, hv_TrainSamples, hv_TrainResult;
  HTuple  hv_ErrorHint, hv_ExceptionThrown, hv_BatchSize;

  //
  //This procedure sets the batch size of the deep-learning-based model DLModelHandle
  //to the maximum possible value given the current GPU memory.
  //
  GetDlModelParam(hv_DLModelHandle, "type", &hv_ModelType);
  hv_ModelTypesSupported.Clear();
  hv_ModelTypesSupported[0] = "classification";
  hv_ModelTypesSupported[1] = "detection";
  hv_ModelTypesSupported[2] = "gc_anomaly_detection";
  hv_ModelTypesSupported[3] = "ocr_detection";
  hv_ModelTypesSupported[4] = "ocr_recognition";
  hv_ModelTypesSupported[5] = "segmentation";
  TupleFindFirst(hv_ModelTypesSupported, hv_ModelType, &hv_Index);
  if (0 != (int(hv_Index==-1)))
  {
    throw HException(("Not supported. This procedure does not work for "+hv_ModelType)+" models.");
  }
  //Get the GPU ID set to the model and the associated GPU device.
  //Note that the currently set device does not need to be of type 'gpu'.
  GetDlModelParam(hv_DLModelHandle, "gpu", &hv_GpuID);
  GetDlModelParam(hv_DLModelHandle, "device", &hv_DLDeviceOrig);
  QueryAvailableDlDevices((HTuple("runtime").Append("id")), HTuple("gpu").TupleConcat(hv_GpuID), 
      &hv_DLDeviceGPU);
  try
  {
    //First we free the GPU memory completely to get a clean model.
    //Note, this only works for hardware where 'cpu' is available.
    QueryAvailableDlDevices((HTuple("runtime").Append("id")), (HTuple("cpu").Append(0)), 
        &hv_DLDeviceCPU);
    SetDlModelParam(hv_DLModelHandle, "device", hv_DLDeviceCPU);
    SetDlModelParam(hv_DLModelHandle, "batch_size", 1);
    //
    //Create a copy which we want to use so that we do not modify the original model.
    SerializeDlModel(hv_DLModelHandle, &hv_SerializedItemHandle);
    hv_OriginalModel = hv_DLModelHandle;
    DeserializeDlModel(hv_SerializedItemHandle, &hv_DLModelHandle);
  }
  // catch (Exception) 
  catch (HException &HDevExpDefaultException)
  {
    HDevExpDefaultException.ToHTuple(&hv_Exception);
    //In case no CPU hardware is available we try to find maximum batch size
    //directly.
    hv_OriginalModel = HTuple();
  }
  //
  //Initialize the bounds.
  hv_BatchSizeTestL = 1;
  hv_BatchSizeTestR = hv_BatchSizeUpperBound;
  hv_BatchSizeTest = hv_BatchSizeUpperBound;
  try
  {
    //Set initial batch size to 1. This has to work. Otherwise an exception is thrown.
    SetDlModelParam(hv_DLModelHandle, "batch_size", hv_BatchSizeTestL);
    SetDlModelParam(hv_DLModelHandle, "device", hv_DLDeviceGPU);
    //
    //Even if the batch size setting works it might be that a train step fails
    //because during training more data is needed.
    //
    //Hence, generate blank train samples here.
    gen_blank_dl_train_samples(hv_DLModelHandle, &hv_TrainSamples);
    //
    //Do a few train steps to get the training data initialized.
    TrainDlModelBatch(hv_DLModelHandle, hv_TrainSamples, &hv_TrainResult);
    TrainDlModelBatch(hv_DLModelHandle, hv_TrainSamples, &hv_TrainResult);
    TrainDlModelBatch(hv_DLModelHandle, hv_TrainSamples, &hv_TrainResult);
  }
  // catch (Exception) 
  catch (HException &HDevExpDefaultException)
  {
    HDevExpDefaultException.ToHTuple(&hv_Exception);
    if (0 != (HTuple(HTuple(int(HTuple(hv_Exception[0])!=4104)).TupleAnd(int(HTuple(hv_Exception[0])!=4201))).TupleAnd(int(HTuple(hv_Exception[0])!=4206))))
    {
      //Not an out of CUDA memory or cuDNN error. Hence, throw.
      throw HException(hv_Exception);
    }
    if (0 != (HTuple(HTuple(HTuple(int(hv_ModelType==HTuple("classification"))).TupleOr(int(hv_ModelType==HTuple("detection")))).TupleOr(int(hv_ModelType==HTuple("segmentation")))).TupleOr(int(hv_ModelType==HTuple("ocr_detection")))))
    {
      hv_ErrorHint = "Try to reduce image_dimension or use a GPU with more RAM.";
    }
    else if (0 != (int(hv_ModelType==HTuple("gc_anomaly_detection"))))
    {
      hv_ErrorHint = HTuple("Try to reduce image_dimension, increase the model's patch_size or use a GPU with more RAM. Setting 'cudnn_deterministic' to 'true' with set_system may help as well.");
    }
    else
    {
      hv_ErrorHint = "Try to use a GPU with more RAM.";
    }
    throw HException("Error while trying to find the maximum batch size. Even the minimum (1) does not fit into memory. "+hv_ErrorHint);
    return;
  }
  //
  //Loop as long as we have not found the maximum batch size for the available GPU memory.
  while (0 != (int((hv_BatchSizeTestR-hv_BatchSizeTestL)>0)))
  {
    //
    //Check if an exception is thrown due to out of memory errors.
    hv_ExceptionThrown = 0;
    try
    {
      //Try to set the batch size.
      SetDlModelParam(hv_DLModelHandle, "batch_size", hv_BatchSizeTest);
      //
      //Even if the batch size setting works it might be that a train step fails
      //because during training more data is needed.
      //
      //Hence, generate blank train samples here.
      gen_blank_dl_train_samples(hv_DLModelHandle, &hv_TrainSamples);
      //
      //Do a few train steps to get the training data initialized.
      TrainDlModelBatch(hv_DLModelHandle, hv_TrainSamples, &hv_TrainResult);
      TrainDlModelBatch(hv_DLModelHandle, hv_TrainSamples, &hv_TrainResult);
      TrainDlModelBatch(hv_DLModelHandle, hv_TrainSamples, &hv_TrainResult);
    }
    // catch (Exception) 
    catch (HException &HDevExpDefaultException)
    {
      HDevExpDefaultException.ToHTuple(&hv_Exception);
      if (0 != (HTuple(int(HTuple(hv_Exception[0])!=4104)).TupleAnd(int(HTuple(hv_Exception[0])!=4201))))
      {
        //Not an out of CUDA memory or cuDNN error. Hence, throw.
        throw HException(hv_Exception);
      }
      //Update the upper bound.
      hv_BatchSizeTestR = hv_BatchSizeTest-1;
      hv_ExceptionThrown = 1;
    }
    if (0 != (hv_ExceptionThrown.TupleNot()))
    {
      //Update the lower bound.
      hv_BatchSizeTestL = hv_BatchSizeTest;
    }
    //Continue with next test.
    hv_BatchSizeTest = hv_BatchSizeTestL+((((hv_BatchSizeTestR-hv_BatchSizeTestL)/2.0).TupleCeil()).TupleInt());
  }
  hv_BatchSize = hv_BatchSizeTest;
  //In case we used a copy we clear it.
  if (0 != (int((hv_OriginalModel.TupleLength())>0)))
  {
    ClearDlModel(hv_DLModelHandle);
    //Use the original model again.
    hv_DLModelHandle = hv_OriginalModel;
  }
  SetDlModelParam(hv_DLModelHandle, "device", hv_DLDeviceOrig);
  SetDlModelParam(hv_DLModelHandle, "batch_size", hv_BatchSize);
  //
  return;
}


